{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Emotion with Azure Cognitive Services\n",
    "\n",
    "Azure Cognitive Services (ACS) provides an API for analyzing images and detecting the emotions on faces in those images. This notebook uses that service to produce a report of the dominant emotion detected on each face in any image provided by a link to that image.\n",
    "\n",
    "## Imports\n",
    "\n",
    "First, you must import the packages necessary for this demonstration. You can run code in this notebook by pressing `Shift`+`Enter` when your cursor is in the cell. We will be doing this a lot in this demonstration. Try it by clicking in the cell below and running the code. It should print \"All imports were successful\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports were successful!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import key_handler\n",
    "import json\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import patches\n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "print(\"All imports were successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Azure Cognitive Services\n",
    "\n",
    "This demonstration connects to a RESTful ACS endpoint. The `api_url` is the general endpoint for all ACS services, you append `/face/v1.0/detect` to tell ACS which service you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = \"https://australiaeast.api.cognitive.microsoft.com\"\n",
    "face_detect_endpoint_url = api_url + \"/face/v1.0/detect\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Example Images\n",
    "\n",
    "We have selected some example images for you to test emotion recognition. You can also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_0 = \"https://i.imgur.com/aegNK4W.jpg\"\n",
    "example_1 = \"https://i.imgur.com/RLSN9rx.jpg\"\n",
    "example_2 = \"https://i.imgur.com/vEuLg5n.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Emotion Analysis Function\n",
    "\n",
    "The function below uses the `requests` package to make an HTTP call to the ACS API. It returns a `JSON` with the position of faces in the image and their emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image(image_url):\n",
    "    \"\"\"\n",
    "    Calls to the Azure Cognitive Services Face Recognition API.\n",
    "    Returns a list of json, one for each face with positions in the image and their emotions.\n",
    "    \"\"\"\n",
    "    response = requests.post(\n",
    "        face_detect_endpoint_url,\n",
    "        params={\"returnFaceAttributes\": \"emotion\"},\n",
    "        headers={\"Ocp-Apim-Subscription-Key\": key_handler.get_api_key()},\n",
    "        json={\"url\": image_url},\n",
    "    )\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to see the raw output returned from the API. Change the `image_url` to an image of your choice to see what ACS returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "You may not have created a `local.settings.json` file. See LAB_SETUP.md for instructions.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Documents\\Code\\azure-python-labs-pr\\2-Finding-Faces-With-Python-And-Azure-Cognitive-Services\\key_handler\\__init__.py\u001b[0m in \u001b[0;36mget_api_key\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local.settings.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msettings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"key1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'local.settings.json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5aed7c85bce7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mimage_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexample_0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfaces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyze_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalyze_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-edeb1ed15807>\u001b[0m in \u001b[0;36manalyze_image\u001b[1;34m(image_url)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mface_detect_endpoint_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"returnFaceAttributes\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"emotion\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"Ocp-Apim-Subscription-Key\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mkey_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_api_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mjson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"url\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mimage_url\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     )\n",
      "\u001b[1;32m~\\Documents\\Code\\azure-python-labs-pr\\2-Finding-Faces-With-Python-And-Azure-Cognitive-Services\\key_handler\\__init__.py\u001b[0m in \u001b[0;36mget_api_key\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         raise FileNotFoundError(\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[1;34m\"You may not have created a `local.settings.json` file. See LAB_SETUP.md for instructions.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         )\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: You may not have created a `local.settings.json` file. See LAB_SETUP.md for instructions."
     ]
    }
   ],
   "source": [
    "image_url = example_0\n",
    "faces = analyze_image(image_url)\n",
    "print(analyze_image(image_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate the Images\n",
    "\n",
    "This function makes use of the `analyze_image` function from above to plot the image with and overlay of faces and the dominant emotion detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dominant_emotion(emotions):\n",
    "    return max(emotions, key=emotions.get).capitalize()\n",
    "\n",
    "\n",
    "def get_face_patch(face_rectangle):\n",
    "    origin = (face_rectangle[\"left\"], face_rectangle[\"top\"])\n",
    "    width, height = face_rectangle[\"width\"], face_rectangle[\"height\"]\n",
    "    return patches.Rectangle(origin, width, height, fill=False, linewidth=2, color=\"w\")\n",
    "\n",
    "\n",
    "def annotate_faces(image_url):\n",
    "    faces = analyze_image(image_url)\n",
    "\n",
    "    # Save the image for plotting.\n",
    "    image_file = BytesIO(requests.get(image_url).content)\n",
    "    image = Image.open(image_file)\n",
    "\n",
    "    # Plot the base image\n",
    "    plt.figure(figsize=(18, 18))\n",
    "    plt.text(0, 0, f\"{len(faces)} faces\", fontsize=18, backgroundcolor=\"w\")\n",
    "    plt.axis(\"off\")\n",
    "    ax = plt.imshow(image, alpha=0.8)\n",
    "\n",
    "    # Annotate each face returned by Azure Cognitive Services\n",
    "    for face in faces:\n",
    "        ax.axes.add_patch(get_face_patch(face[\"faceRectangle\"]))\n",
    "        dominant_emotion = get_dominant_emotion(face[\"faceAttributes\"][\"emotion\"])\n",
    "        left_anchor, top_anchor = (\n",
    "            face[\"faceRectangle\"][\"left\"],\n",
    "            face[\"faceRectangle\"][\"top\"],\n",
    "        )\n",
    "        plt.text(\n",
    "            left_anchor, top_anchor, dominant_emotion, fontsize=18, backgroundcolor=\"w\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it Yourself\n",
    "\n",
    "You can run the code to produce anotated images by calling `annotate_faces` with an image url as an argument. Try it with any image url you find!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotate_faces(example_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Other Cognitive Services API\n",
    "\n",
    "Explore the [directory](https://azure.microsoft.com/en-us/services/cognitive-services/directory/) of Cognitive Service APIs to learn more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
